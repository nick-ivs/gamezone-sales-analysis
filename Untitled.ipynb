{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "100a0460-5aa0-4cc0-bf7e-03d452dc0247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: README.md\n",
      "Created: .gitignore\n",
      "Created: requirements.txt\n",
      "Created: notebooks/01_data_exploration.py\n",
      "Created: notebooks/02_data_cleaning.py\n",
      "Created: notebooks/03_sales_and_ltv.py\n",
      "Created: notebooks/04_churn_indicators.py\n",
      "Created: notebooks/05_product_insights.py\n",
      "\n",
      "All scaffolding files written. You can now upload this folder to GitHub.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import date\n",
    "\n",
    "# ---------- 1. Create folders ----------\n",
    "os.makedirs(\"notebooks\", exist_ok=True)\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"assets\", exist_ok=True)\n",
    "\n",
    "# ---------- 2. File contents ----------\n",
    "\n",
    "readme_md = f\"\"\"# Gamezone Sales & Customer Behavior Analysis\n",
    "\n",
    "This project is an end-to-end analytics case study built with **Python** and **BigQuery**.\n",
    "\n",
    "It explores:\n",
    "\n",
    "- Sales trends and the early-2021 drop\n",
    "- Customer behavior and **Lifetime Value (LTV)**\n",
    "- **Churn indicators** based on recency and frequency\n",
    "- Product performance and co-purchase patterns\n",
    "- Impact of **marketing channels** and **platforms** on revenue\n",
    "\n",
    "## ðŸ“¦ Dataset\n",
    "\n",
    "Schema (BigQuery table `Gamezone.Orders`):\n",
    "\n",
    "- `USER_ID` â€“ unique customer identifier\n",
    "- `ORDER_ID` â€“ unique order identifier\n",
    "- `PURCHASE_TS` â€“ timestamp of purchase\n",
    "- `SHIP_TS` â€“ shipping date\n",
    "- `PRODUCT_NAME` â€“ name of purchased product\n",
    "- `PRODUCT_ID` â€“ product identifier\n",
    "- `USD_PRICE` â€“ purchase value in USD\n",
    "- `PURCHASE_PLATFORM` â€“ platform used (e.g. web, mobile app)\n",
    "- `MARKETING_CHANNEL` â€“ marketing/acquisition channel\n",
    "- `ACCOUNT_CREATION_METHOD` â€“ how the account was created\n",
    "- `COUNTRY_CODE` â€“ customer country (ISO-like code)\n",
    "\n",
    "> Note: The live dataset sits in BigQuery. This repo focuses on **analysis code** and can use a small sample in `data/` if needed.\n",
    "\n",
    "## ðŸ›  Tech Stack\n",
    "\n",
    "- Python\n",
    "- pandas\n",
    "- matplotlib\n",
    "- google-cloud-bigquery\n",
    "- db-dtypes\n",
    "- Jupyter Notebook\n",
    "\n",
    "## ðŸ“š Notebooks / Scripts\n",
    "\n",
    "Suggested structure:\n",
    "\n",
    "- `notebooks/01_data_exploration.py`  \n",
    "  - Connect to BigQuery, load table, basic EDA  \n",
    "- `notebooks/02_data_cleaning.py`  \n",
    "  - Type conversions, text normalization, flagging issues  \n",
    "- `notebooks/03_sales_and_ltv.py`  \n",
    "  - Sales trends, LTV calculation, LTV segmentation  \n",
    "- `notebooks/04_churn_indicators.py`  \n",
    "  - Recency, frequency, churn threshold and segmentation  \n",
    "- `notebooks/05_product_insights.py`  \n",
    "  - Product performance, volatility, co-purchase analysis  \n",
    "\n",
    "You can convert these scripts to `.ipynb` notebooks or use them directly in Jupyter.\n",
    "\n",
    "## ðŸ” Key Analytics Topics Covered\n",
    "\n",
    "- Data cleaning:\n",
    "  - Normalize text fields\n",
    "  - Parse timestamps and compute shipping delays\n",
    "  - Flag \"ship before purchase\" anomalies\n",
    "- Sales analytics:\n",
    "  - Daily and monthly revenue trends\n",
    "  - Channel and platform breakdowns\n",
    "- Customer analytics:\n",
    "  - LTV per user\n",
    "  - RFM-style features (recency, frequency, monetary)\n",
    "  - Churn indicators based on inactivity windows\n",
    "- Product analytics:\n",
    "  - Top and bottom performers\n",
    "  - Volatility and seasonality\n",
    "  - Basic co-purchase analysis using composite orders (user + day)\n",
    "\n",
    "## â–¶ï¸ How to Use\n",
    "\n",
    "1. Install dependencies:\n",
    "\n",
    "   - See `requirements.txt`\n",
    "\n",
    "2. Set up BigQuery credentials:\n",
    "\n",
    "   - Use a service account JSON key and set `GOOGLE_APPLICATION_CREDENTIALS`\n",
    "   - Point to your GCP project and dataset in the code\n",
    "\n",
    "3. Run the notebooks / scripts in `notebooks/` in order.\n",
    "\n",
    "## ðŸ“… Last Updated\n",
    "\n",
    "- {date.today().isoformat()}\n",
    "\"\"\"\n",
    "\n",
    "gitignore_txt = r\"\"\"# Python\n",
    "__pycache__/\n",
    "*.py[cod]\n",
    "*.pyo\n",
    "*.pyd\n",
    "*.pkl\n",
    "*.log\n",
    "\n",
    "# Virtual environments\n",
    ".env\n",
    ".venv\n",
    "venv/\n",
    "env/\n",
    "\n",
    "# Jupyter\n",
    ".ipynb_checkpoints/\n",
    "jupyter_notebook_config.py\n",
    "\n",
    "# Data\n",
    "data/\n",
    "*.parquet\n",
    "*.csv\n",
    "*.xlsx\n",
    "\n",
    "# OS / Editor\n",
    ".DS_Store\n",
    "Thumbs.db\n",
    ".vscode/\n",
    ".idea/\n",
    "\"\"\"\n",
    "\n",
    "requirements_txt = \"\"\"pandas\n",
    "matplotlib\n",
    "google-cloud-bigquery\n",
    "db-dtypes\n",
    "jupyter\n",
    "\"\"\"\n",
    "\n",
    "nb_01_exploration = \"\"\"\\\"\\\"\\\"01_data_exploration.py\n",
    "Initial data exploration for Gamezone orders dataset.\n",
    "- Connects to BigQuery\n",
    "- Loads Orders table\n",
    "- Basic info(), head(), descriptive stats\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# TODO: update this path to your JSON key\n",
    "# os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = r\"C:\\\\path\\\\to\\\\service-account.json\"\n",
    "\n",
    "PROJECT_ID = \"wise-bongo-476621-k6\"  # change if needed\n",
    "DATASET_ID = \"Gamezone\"\n",
    "TABLE_ID = \"Orders\"\n",
    "\n",
    "def load_orders(limit=None):\n",
    "    client = bigquery.Client(project=PROJECT_ID)\n",
    "    query = f\\\"\\\"\\\"\n",
    "        SELECT *\n",
    "        FROM `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`\n",
    "        {f\"LIMIT {limit}\" if limit else \"\"}\n",
    "    \\\"\\\"\\\"\n",
    "    df = client.query(query).to_dataframe()\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = load_orders()\n",
    "    print(\"Shape:\", df.shape)\n",
    "    print(\"\\\\nDtypes:\")\n",
    "    print(df.dtypes)\n",
    "    print(\"\\\\nHead:\")\n",
    "    print(df.head())\n",
    "\"\"\"\n",
    "\n",
    "nb_02_cleaning = \"\"\"\\\"\\\"\\\"02_data_cleaning.py\n",
    "Cleaning pipeline for Gamezone Orders:\n",
    "- Normalize text fields\n",
    "- Convert timestamps\n",
    "- Flag ship-before-purchase\n",
    "- Save cleaned snapshot to parquet\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "INPUT_PARQUET = \"data/orders_raw.parquet\"\n",
    "OUTPUT_PARQUET = \"data/orders_clean.parquet\"\n",
    "\n",
    "NULL_TOKENS = {\n",
    "    \"\", \"none\", \"n/a\", \"na\", \"null\", \"unknown\", \"undefined\",\n",
    "    \"not available\", \"not applicable\"\n",
    "}\n",
    "\n",
    "def normalize_text_series(s: pd.Series) -> pd.Series:\n",
    "    s = s.astype(\"string\")\n",
    "    out = s.str.strip()\n",
    "    out = out.str.replace(r\"\\\\s+\", \" \", regex=True)\n",
    "    out = out.str.lower()\n",
    "    out = out.where(~out.isin(NULL_TOKENS), pd.NA)\n",
    "    return out\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(INPUT_PARQUET):\n",
    "        raise FileNotFoundError(f\"{INPUT_PARQUET} not found. Export raw data first.\")\n",
    "\n",
    "    df = pd.read_parquet(INPUT_PARQUET)\n",
    "\n",
    "    # Normalize selected text columns\n",
    "    text_cols = [\n",
    "        \"PURCHASE_PLATFORM\",\n",
    "        \"MARKETING_CHANNEL\",\n",
    "        \"ACCOUNT_CREATION_METHOD\",\n",
    "        \"COUNTRY_CODE\",\n",
    "        \"PRODUCT_NAME\",\n",
    "    ]\n",
    "    for col in text_cols:\n",
    "        if col in df.columns:\n",
    "            before = df[col].copy()\n",
    "            after = normalize_text_series(before)\n",
    "            changes = (before.astype(\"string\").fillna(\"<NA>\") != after.astype(\"string\").fillna(\"<NA>\")).sum()\n",
    "            df[col] = after\n",
    "            print(f\"{col}: {changes} values normalized\")\n",
    "\n",
    "    # Convert timestamps\n",
    "    df[\"PURCHASE_TS\"] = pd.to_datetime(df[\"PURCHASE_TS\"], errors=\"coerce\", utc=True)\n",
    "    df[\"SHIP_TS_dt\"] = pd.to_datetime(df[\"SHIP_TS\"], errors=\"coerce\", utc=True)\n",
    "    df[\"SHIP_TS_DATE\"] = df[\"SHIP_TS_dt\"].dt.date\n",
    "\n",
    "    # Flag ship-before-purchase\n",
    "    mask_ship_before = df[\"SHIP_TS_dt\"] < df[\"PURCHASE_TS\"]\n",
    "    df[\"FLAG_SHIP_BEFORE_PURCHASE\"] = mask_ship_before\n",
    "    print(\"Ship-before-purchase rows:\", int(mask_ship_before.sum()))\n",
    "\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    df.to_parquet(OUTPUT_PARQUET)\n",
    "    print(\"Saved cleaned data to\", OUTPUT_PARQUET)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\"\n",
    "\n",
    "nb_03_sales_ltv = \"\"\"\\\"\\\"\\\"03_sales_and_ltv.py\n",
    "Sales trend and LTV analysis:\n",
    "- Sales over time\n",
    "- Basic LTV per user\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_PATH = \"data/orders_clean.parquet\"\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(DATA_PATH):\n",
    "        raise FileNotFoundError(f\"{DATA_PATH} not found. Run 02_data_cleaning.py first.\")\n",
    "\n",
    "    df = pd.read_parquet(DATA_PATH)\n",
    "    df[\"PURCHASE_TS\"] = pd.to_datetime(df[\"PURCHASE_TS\"], errors=\"coerce\", utc=True)\n",
    "    df[\"USD_PRICE\"] = pd.to_numeric(df[\"USD_PRICE\"], errors=\"coerce\")\n",
    "\n",
    "    # Daily sales\n",
    "    df[\"purchase_date\"] = df[\"PURCHASE_TS\"].dt.date\n",
    "    daily = (\n",
    "        df.groupby(\"purchase_date\")[\"USD_PRICE\"]\n",
    "        .sum()\n",
    "        .reset_index(name=\"total_sales_usd\")\n",
    "        .sort_values(\"purchase_date\")\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(daily[\"purchase_date\"], daily[\"total_sales_usd\"])\n",
    "    plt.title(\"Daily Sales Trend (USD)\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Total Sales (USD)\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # LTV per user\n",
    "    ltv = (\n",
    "        df.groupby(\"USER_ID\")[\"USD_PRICE\"]\n",
    "        .sum()\n",
    "        .reset_index(name=\"LTV_USD\")\n",
    "        .sort_values(\"LTV_USD\", ascending=False)\n",
    "    )\n",
    "    print(\"Top 10 customers by LTV:\")\n",
    "    print(ltv.head(10))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\"\n",
    "\n",
    "nb_04_churn = \"\"\"\\\"\\\"\\\"04_churn_indicators.py\n",
    "Churn analysis using simple RFM-style features:\n",
    "- Recency (days since last purchase)\n",
    "- Frequency (number of orders)\n",
    "- Monetary (total spend)\n",
    "- Churn flag based on inactivity threshold\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_PATH = \"data/orders_clean.parquet\"\n",
    "CHURN_THRESHOLD_DAYS = 90\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(DATA_PATH):\n",
    "        raise FileNotFoundError(f\"{DATA_PATH} not found. Run 02_data_cleaning.py first.\")\n",
    "\n",
    "    df = pd.read_parquet(DATA_PATH)\n",
    "    df[\"PURCHASE_TS\"] = pd.to_datetime(df[\"PURCHASE_TS\"], errors=\"coerce\", utc=True)\n",
    "\n",
    "    snapshot_date = df[\"PURCHASE_TS\"].max()\n",
    "    print(\"Snapshot date:\", snapshot_date)\n",
    "\n",
    "    rfm = (\n",
    "        df.groupby(\"USER_ID\").agg(\n",
    "            last_purchase=(\"PURCHASE_TS\", \"max\"),\n",
    "            frequency=(\"ORDER_ID\", \"nunique\"),\n",
    "            monetary_value=(\"USD_PRICE\", \"sum\"),\n",
    "        )\n",
    "    ).reset_index()\n",
    "\n",
    "    rfm[\"recency_days\"] = (snapshot_date - rfm[\"last_purchase\"]).dt.days\n",
    "    rfm[\"churned\"] = (rfm[\"recency_days\"] > CHURN_THRESHOLD_DAYS).astype(int)\n",
    "\n",
    "    print(\"Sample RFM rows:\")\n",
    "    print(rfm.head())\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    rfm[\"recency_days\"].plot.hist(bins=50)\n",
    "    plt.axvline(CHURN_THRESHOLD_DAYS, color=\"red\", linestyle=\"--\", label=\"Churn threshold\")\n",
    "    plt.title(\"Recency Distribution\")\n",
    "    plt.xlabel(\"Days Since Last Purchase\")\n",
    "    plt.ylabel(\"Number of Customers\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\"\n",
    "\n",
    "nb_05_products = \"\"\"\\\"\\\"\\\"05_product_insights.py\n",
    "Product insights:\n",
    "- Top products by revenue\n",
    "- Monthly trend for top products\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_PATH = \"data/orders_clean.parquet\"\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(DATA_PATH):\n",
    "        raise FileNotFoundError(f\"{DATA_PATH} not found. Run 02_data_cleaning.py first.\")\n",
    "\n",
    "    df = pd.read_parquet(DATA_PATH)\n",
    "    df[\"USD_PRICE\"] = pd.to_numeric(df[\"USD_PRICE\"], errors=\"coerce\")\n",
    "    df[\"PURCHASE_TS\"] = pd.to_datetime(df[\"PURCHASE_TS\"], errors=\"coerce\", utc=True)\n",
    "    df[\"month\"] = df[\"PURCHASE_TS\"].dt.to_period(\"M\").dt.to_timestamp()\n",
    "\n",
    "    product_sales = (\n",
    "        df.groupby(\"PRODUCT_NAME\")[\"USD_PRICE\"]\n",
    "        .sum()\n",
    "        .reset_index(name=\"total_sales_usd\")\n",
    "        .sort_values(\"total_sales_usd\", ascending=False)\n",
    "    )\n",
    "\n",
    "    print(\"Top 10 products by revenue:\")\n",
    "    print(product_sales.head(10))\n",
    "\n",
    "    top_products = product_sales.head(5)[\"PRODUCT_NAME\"].tolist()\n",
    "    trends = (\n",
    "        df[df[\"PRODUCT_NAME\"].isin(top_products)]\n",
    "        .groupby([\"month\", \"PRODUCT_NAME\"])[\"USD_PRICE\"]\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for name, group in trends.groupby(\"PRODUCT_NAME\"):\n",
    "        plt.plot(group[\"month\"], group[\"USD_PRICE\"], marker=\"o\", label=name)\n",
    "    plt.title(\"Monthly Sales Trend for Top Products\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Sales (USD)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\"\n",
    "\n",
    "# ---------- 3. Write files ----------\n",
    "\n",
    "files_to_write = {\n",
    "    \"README.md\": readme_md,\n",
    "    \".gitignore\": gitignore_txt,\n",
    "    \"requirements.txt\": requirements_txt,\n",
    "    \"notebooks/01_data_exploration.py\": nb_01_exploration,\n",
    "    \"notebooks/02_data_cleaning.py\": nb_02_cleaning,\n",
    "    \"notebooks/03_sales_and_ltv.py\": nb_03_sales_ltv,\n",
    "    \"notebooks/04_churn_indicators.py\": nb_04_churn,\n",
    "    \"notebooks/05_product_insights.py\": nb_05_products,\n",
    "}\n",
    "\n",
    "for path, content in files_to_write.items():\n",
    "    folder = os.path.dirname(path)\n",
    "    if folder:\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "    print(\"Created:\", path)\n",
    "\n",
    "print(\"\\nAll scaffolding files written. You can now upload this folder to GitHub.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e02d8a-77d7-464c-9a88-dbfa29b87e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
